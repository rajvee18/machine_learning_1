{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ef7237-87ec-4aeb-b713-cba2b22ecd24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e282d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1cb17f",
   "metadata": {},
   "source": [
    "## **Understanding the Random forest algorithm**\n",
    "\n",
    "The Random Forest algorithm uses the concept of bagging (Bootstrap Aggregating) and random feature selection at each split to train multiple decision trees. It then aggregates their outputs (via voting or averaging) to make robust predictions, and it has built-in methods like the OOB error to estimate model performance. The randomness in data and feature selection introduces diversity in the trees, which reduces the risk of overfitting, making Random Forests a powerful ensemble learning method.\n",
    "\n",
    "\n",
    "### 1. Bootstrapping (Sampling with Replacement)\n",
    "Each tree in the Random Forest is trained on a random subset of the data, selected with replacement. This technique is called **Bootstrap Aggregating** or **Bagging**.\n",
    "\n",
    "- Let $S = \\{x_1, x_2, ..., x_N\\}$ be the original training dataset of size $N$, where $x_i$ represents the $i$-th data point assuming that each data point is a $d$ dimensional vector.\n",
    "- From $S$, we create $T$ bootstrap samples, denoted by $S_1, S_2, ..., S_T$. Each $S_t$ is generated by randomly selecting $N$ samples from $S$ with replacement. Thus, some samples from $S$ may appear multiple times in $S_t$, while others may not appear at all.\n",
    "  \n",
    "### 2. Training Decision Trees\n",
    "For each bootstrap sample $S_t$, a decision tree $T_t$ is trained independently. The key feature of Random Forest is that, at each split of the decision tree, only a random subset of features is considered rather than all features.\n",
    "\n",
    "- Let $d$ be the number of features in the dataset.\n",
    "- At each node of the decision tree, a random subset of $m$ features is chosen to consider for the split, where $m$ is typically $\\sqrt{d}$ for classification (or $\\log_2 d$ for regression). This ensures that the trees are diverse and reduces the risk of overfitting.\n",
    "\n",
    "### 3. Prediction from Individual Trees\n",
    "Once the $T$ decision trees $T_1, T_2, ..., T_T$ are trained, the Random Forest algorithm uses the predictions of these trees to make a final prediction.\n",
    "\n",
    "#### Classification:\n",
    "For a new input $x$, each decision tree $T_t$ produces a predicted class label $\\hat{y}_t$. The final prediction is made by **majority voting**:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{mode}(\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_T)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\hat{y}_t$ is the predicted class label from tree $t$.\n",
    "- The mode operation selects the most frequent predicted class across all trees.\n",
    "\n",
    "#### Regression:\n",
    "For regression tasks, the prediction from each tree is the continuous output $\\hat{y}_t$. The final prediction is the **average** of these predictions:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\frac{1}{T} \\sum_{t=1}^{T} \\hat{y}_t\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\hat{y}_t$ is the predicted value from tree $t$.\n",
    "\n",
    "### 4. Out-of-Bag (OOB) Error Estimation\n",
    "Random Forests have a built-in method to estimate the model's performance called the **out-of-bag (OOB) error**. Since each tree is trained on a bootstrap sample, some of the data is left out of the training set for each tree. This data is called the out-of-bag (OOB) data.\n",
    "\n",
    "- For each data point $x_i$, the OOB error is calculated by using the subset of trees that did not use $x_i$ in their training process to predict the outcome. Let $T_i^{\\text{OOB}}$ represent the set of trees that did not use $x_i$ in their training.\n",
    "\n",
    "- The OOB prediction for $x_i$ is the majority vote (for classification) or the average (for regression) from the trees in $T_i^{\\text{OOB}}$. The error is then calculated as the difference between the true value $y_i$ and the OOB prediction.\n",
    "\n",
    "#### OOB Error (Classification):\n",
    "$$\n",
    "\\text{OOB Error} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}(\\hat{y}_i^{\\text{OOB}} \\neq y_i)\n",
    "$$\n",
    "where $\\hat{y}_i^{\\text{OOB}}$ is the OOB prediction for sample $x_i$, and $\\mathbb{I}(\\cdot)$ is the indicator function.\n",
    "\n",
    "#### OOB Error (Regression):\n",
    "$$\n",
    "\\text{OOB Error} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i^{\\text{OOB}} - y_i)^2\n",
    "$$\n",
    "where $\\hat{y}_i^{\\text{OOB}}$ is the OOB predicted value for sample $x_i$.\n",
    "\n",
    "The OOB error provides a way to estimate the model's generalization error without needing a separate validation set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b676d-b8fd-4919-b1b8-a41d1f95e64e",
   "metadata": {},
   "source": [
    "## Getting toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec436f1e-21f4-440e-9a16-51c0506116fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv('heart_failure_clinical_records_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe1b2394-a6e9-426c-9f5d-4dc0688c4ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n",
       "       'ejection_fraction', 'high_blood_pressure', 'platelets',\n",
       "       'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time',\n",
       "       'DEATH_EVENT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71840d30-584e-47ec-848f-54ad73cb63a3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>anaemia</th>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>ejection_fraction</th>\n",
       "      <th>high_blood_pressure</th>\n",
       "      <th>platelets</th>\n",
       "      <th>serum_creatinine</th>\n",
       "      <th>serum_sodium</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoking</th>\n",
       "      <th>time</th>\n",
       "      <th>DEATH_EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>582</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>265000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7861</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>263358.03</td>\n",
       "      <td>1.1</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>162000.00</td>\n",
       "      <td>1.3</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>210000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>327000.00</td>\n",
       "      <td>2.7</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
       "0  75.0        0                       582         0                 20   \n",
       "1  55.0        0                      7861         0                 38   \n",
       "2  65.0        0                       146         0                 20   \n",
       "3  50.0        1                       111         0                 20   \n",
       "4  65.0        1                       160         1                 20   \n",
       "\n",
       "   high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
       "0                    1  265000.00               1.9           130    1   \n",
       "1                    0  263358.03               1.1           136    1   \n",
       "2                    0  162000.00               1.3           129    1   \n",
       "3                    0  210000.00               1.9           137    1   \n",
       "4                    0  327000.00               2.7           116    0   \n",
       "\n",
       "   smoking  time  DEATH_EVENT  \n",
       "0        0     4            1  \n",
       "1        0     6            1  \n",
       "2        1     7            1  \n",
       "3        0     7            1  \n",
       "4        0     8            1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d2a12fc-8f78-40a8-bf6b-9495ea67ee30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categrical_features=['anaemia','diabetes','high_blood_pressure','sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f508e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n",
    "       'ejection_fraction', 'high_blood_pressure', 'platelets',\n",
    "        'serum_sodium', 'sex', 'smoking', 'time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21d01962-eef8-498b-ba4b-5f2195673bb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data=np.array(d[features])\n",
    "target_1=np.array(d['DEATH_EVENT'])\n",
    "target_2=np.array(d['serum_creatinine'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd12df6a-d8d2-4565-8ae9-cf522ec6ec50",
   "metadata": {},
   "source": [
    "data=(data-np.mean(data,axis=0))/np.std(data,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06754667",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(data, target_1, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbb08ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(data, target_2, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b836be12-0651-40c2-b902-b8a93ff46ee2",
   "metadata": {},
   "source": [
    "## Coding the Random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa37f42a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04de87c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting functions for building a decision tree\n",
    "\n",
    "\n",
    "def split(feature_column, threshold):\n",
    "    \"\"\"\n",
    "    Splits the dataset based on the feature and threshold.\n",
    "    \"\"\"\n",
    "    left_idx = np.where(feature_column <= threshold)[0]\n",
    "    right_idx = np.where(feature_column > threshold)[0]\n",
    "    return left_idx, right_idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def is_pure(y):\n",
    "    \"\"\"\n",
    "    Checks if all targets in the dataset are the same.\n",
    "    \"\"\"\n",
    "    return len(np.unique(y)) == 1\n",
    "\n",
    "\n",
    "\n",
    "def impurity(y, task):\n",
    "    \"\"\"\n",
    "    Calculates the impurity of a subset.\n",
    "    \"\"\"\n",
    "    if task == \"classification\":\n",
    "        # Gini impurity\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        return 1 - np.sum(probabilities ** 2)\n",
    "    elif task == \"regression\":\n",
    "        # Variance\n",
    "        return np.var(y)\n",
    "    \n",
    "    \n",
    "def weighted_impurity(left_y, right_y, y, task):\n",
    "    \"\"\"\n",
    "    Calculates the weighted impurity of a split.\n",
    "    \"\"\"\n",
    "    total_samples = len(left_y) + len(right_y)\n",
    "    left_weight = len(left_y) / total_samples\n",
    "    right_weight = len(right_y) / total_samples\n",
    "    \n",
    "    return (left_weight * impurity(y[left_y], task)) + (right_weight * impurity(y[right_y], task))\n",
    "\n",
    "\n",
    "def find_best_split(X, y, task):\n",
    "    \"\"\"\n",
    "    Finds the best feature and threshold to split on.\n",
    "    \n",
    "    Parameters:\n",
    "        X: numpy array of shape (n_samples, n_features)\n",
    "        y: numpy array of shape (n_samples,)\n",
    "        task: \"classification\" or \"regression\"\n",
    "    \n",
    "    Returns:\n",
    "        feature: index of the best feature\n",
    "        threshold: value of the best threshold\n",
    "        left_idx: indices of samples in the left split\n",
    "        right_idx: indices of samples in the right split\n",
    "    \"\"\"\n",
    "    best_feature, best_threshold = None, None\n",
    "    best_impurity = float(\"inf\")\n",
    "    best_left_idx, best_right_idx = None, None\n",
    "    \n",
    "    for feature in range(X.shape[1]):\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "        for threshold in thresholds:\n",
    "            left_idx, right_idx = split(X[:, feature], threshold)\n",
    "            \n",
    "            if len(left_idx) == 0 or len(right_idx) == 0:\n",
    "                continue\n",
    "            \n",
    "            impurity = weighted_impurity(left_idx, right_idx, y, task) \n",
    "\n",
    "            ## The higher the impurity is the lower the deltaI as discussed in the formula\n",
    "            \n",
    "            if impurity < best_impurity:\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "                best_impurity = impurity\n",
    "                best_left_idx, best_right_idx = left_idx, right_idx\n",
    "    \n",
    "    return best_feature, best_threshold, best_left_idx, best_right_idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_leaf(y, task):\n",
    "    \"\"\"\n",
    "    Creates a leaf node.\n",
    "    \"\"\"\n",
    "    if task == \"classification\":\n",
    "        # Majority class\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        return classes[np.argmax(counts)]\n",
    "    elif task == \"regression\":\n",
    "        # Mean value\n",
    "        return np.mean(y)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def build_tree(X, y, depth=0, max_depth=7, min_samples_split=2, task=\"classification\"):\n",
    "    \"\"\"\n",
    "    Recursively builds a decision tree.\n",
    "    \n",
    "    Parameters:\n",
    "        X: numpy array of shape (n_samples, n_features)\n",
    "        y: numpy array of shape (n_samples,)\n",
    "        depth: current depth of the tree\n",
    "        max_depth: maximum depth of the tree\n",
    "        min_samples_split: minimum number of samples required to split\n",
    "        task: \"classification\" or \"regression\"\n",
    "    \n",
    "    Returns:\n",
    "        A tree represented as a nested dictionary or a leaf value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Stopping criteria\n",
    "    if (max_depth is not None and depth >= max_depth) or len(y) < min_samples_split or is_pure(y):\n",
    "        return create_leaf(y, task)\n",
    "    \n",
    "    # Find the best split\n",
    "    feature, threshold, left_idx, right_idx = find_best_split(X, y, task)\n",
    "    \n",
    "    if feature is None:\n",
    "        return create_leaf(y, task)\n",
    "    \n",
    "    # Recursively build left and right branches\n",
    "    left_tree = build_tree(X[left_idx], y[left_idx], depth + 1, max_depth, min_samples_split, task)\n",
    "    right_tree = build_tree(X[right_idx], y[right_idx], depth + 1, max_depth, min_samples_split, task)\n",
    "    \n",
    "    # Return the decision node\n",
    "    return {\n",
    "        \"feature\": feature,\n",
    "        \"threshold\": threshold,\n",
    "        \"left\": left_tree,\n",
    "        \"right\": right_tree\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(sample, tree):\n",
    "    \"\"\"\n",
    "    Predicts the output for a single sample using the tree.\n",
    "    \"\"\"\n",
    "    if not isinstance(tree, dict):  # If tree is not a dictionary object the code runs into this loop\n",
    "        return tree\n",
    "    \n",
    "    feature = tree[\"feature\"]\n",
    "    threshold = tree[\"threshold\"]\n",
    "    \n",
    "    if sample[feature] <= threshold:\n",
    "        return predict(sample, tree[\"left\"])\n",
    "    else:\n",
    "        return predict(sample, tree[\"right\"])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def predict_all(X, tree):\n",
    "    \"\"\"\n",
    "    Predicts the output for multiple samples.\n",
    "    \"\"\"\n",
    "    return np.array([predict(sample, tree) for sample in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db90b52",
   "metadata": {},
   "source": [
    "# Write your own Random Forest Classifier (Coding Challenge)\n",
    "\n",
    "A Random Forest is an ensemble learning method that builds multiple decision trees and combines their predictions for better accuracy. Your task is to implement a simplified version of a Random Forest classifier using a predefined decision tree model (see above). Follow the steps below to write the necessary functions:\n",
    "\n",
    "## 1. Bootstrapping Data  \n",
    "- Implement a function `get_bootstrapped_data(X, y, size=100)` that selects `size` random samples (with replacement) from the dataset `(X, y)`.  \n",
    "- Use `np.random.choice()` to generate random indices and return the corresponding samples from `X` and `y`.  \n",
    "- For simplicity you can retain the entire feature space while generating a bootstrapped sample from the dataset.\n",
    "\n",
    "## 2. Fitting the Random Forest  \n",
    "- Implement a function `fit_random_forest(X, y, number_of_trees=300)`.  \n",
    "- Initialize an empty list to store decision trees.  \n",
    "- For each tree:\n",
    "  - Create a bootstrapped dataset using `get_bootstrapped_data()`.  \n",
    "  - Train a decision tree using `build_tree()` on this dataset.  \n",
    "  - Store the trained tree in the list.  \n",
    "- Return the list of trained decision trees.  \n",
    "\n",
    "## 3. Making Predictions  \n",
    "- Implement a function `predict_random_forest(X, trees)`.  \n",
    "- For each decision tree in the forest:  \n",
    "  - Use `predict_all()` to get predictions on `X`.  \n",
    "  - Store these predictions in a list or NumPy array.  \n",
    "- Determine the final prediction for each sample by taking the most common class (mode) across all trees.  \n",
    "- Return the final predictions.  \n",
    "\n",
    " \n",
    "Step 1: Write the necessary Python functions based on these instructions. \n",
    "Step 2: Calculate out-of-bag error for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb679926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
